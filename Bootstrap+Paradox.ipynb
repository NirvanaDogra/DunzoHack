{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U textblob\n",
    "!pip3 install -U rake-nltk\n",
    "!python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt): #convert to lower case, remove stopwords, get important entities\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize as tokenize\n",
    "    \n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    t_list, key_phrases, entities = [],[],[]\n",
    "    txt_tokens=tokenize(txt)\n",
    "    \n",
    "    t_list = [word.lower() for word in txt_tokens if not word in stop_words]\n",
    "    print(\"After removing stopwords :\\n\",t_list) #return this\n",
    "    \n",
    "    pre_processed_string = \"\"\n",
    "    for word in t_list :\n",
    "        pre_processed_string=pre_processed_string+word+\" \"\n",
    "    pre_processed_string.strip()\n",
    "    print(\"Preprocessed String : \\n\",pre_processed_string)\n",
    "    \n",
    "    from rake_nltk import Rake\n",
    "    \n",
    "    r=Rake()\n",
    "    r.extract_keywords_from_text(txt)\n",
    "    key_phrases = r.get_ranked_phrases()\n",
    "    print(\"Key Phrases : \\n\",key_phrases) #return this\n",
    "    \n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    from collections import Counter\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load()\n",
    "    \n",
    "    doc = nlp(txt)\n",
    "    entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    if len(entities) != 0:\n",
    "        print(\"Entities : \\n\",entities) #return this\n",
    "    else:\n",
    "        print(\"No Entities\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_from_article(url):\n",
    "    \n",
    "    import spacy\n",
    "    import requests\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information(url):\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    extractor=information_from_article(url)\n",
    "    article = nlp(extractor)\n",
    "    print(\"Number of entities in the page : \",len(article.ents)) #return this\n",
    "    labels = [x.label_ for x in article.ents]\n",
    "    \n",
    "    i = len(labels)//3 #30% of the top label count\n",
    "    items = [x.text for x in article.ents]\n",
    "    common_items = [] #Counter(items).most_common(i)\n",
    "    too_common=[x for x in range(5)]\n",
    "    for item in Counter(items).most_common(i):\n",
    "        try:\n",
    "            if int(str(item).split(',')[1].split(')')[0]) in too_common:\n",
    "                continue\n",
    "            else:\n",
    "                common_items.append(str(item).split(',')[0])\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        \n",
    "    sentences = [x for x in article.sents]\n",
    "    print(len(sentences))\n",
    "    print(\"Most important terms in the document : \\n\",common_items) #return this\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        displacy.render(nlp(str(sentences[i])), jupyter=True, style='ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information(\"https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"THe foo d here is amazing expecially the maggi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
